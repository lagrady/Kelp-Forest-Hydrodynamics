{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import vector_tools as vt\n",
    "#import khFunctions as khf\n",
    "%matplotlib ipympl\n",
    "from datetime import timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT, FLAG, AND CLEAN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use .dat, .vhd. and .sen files to generate a dataset that matches format used by \n",
    "#Wheeler and Giddings 2023 to be usable in their functions\n",
    "datfile1 = 'ADV/DEP204.dat'\n",
    "vhdfile1 = 'ADV/DEP204.vhd'\n",
    "senfile1 = 'ADV/DEP204.sen'\n",
    "\n",
    "datfile2 = 'ADV/DEP205.dat'\n",
    "vhdfile2 = 'ADV/DEP205.vhd'\n",
    "senfile2 = 'ADV/DEP205.sen'\n",
    "\n",
    "fs = 32\n",
    "\n",
    "#Create the raw dataset \n",
    "adv1 = vt.vector_to_ds(datfile1, vhdfile1, senfile1, fs)\n",
    "\n",
    "#Flag the raw dataset\n",
    "adv1_flagged = vt.vectorFlag(adv1)\n",
    "\n",
    "#Trim the bad times during adv deployment and recovery\n",
    "adv1_trimmed = adv1_flagged.sel(dict(time=slice('2022-08-02T11:00:00.000000000', '2022-08-11T11:00:00.000000000'),\n",
    "                                     time_sen=slice('2022-08-02T11:00:00.000000000', '2022-08-11T11:00:00.000000000'),\n",
    "                                     time_start=slice('2022-08-02T11:00:00.000000000', '2022-08-11T11:00:00.000000000')))\n",
    "\n",
    "#Repeat with second deployment\n",
    "adv2 = vt.vector_to_ds(datfile2, vhdfile2, senfile2, fs)\n",
    "adv2_flagged = vt.vectorFlag(adv2)\n",
    "adv2_trimmed = adv2_flagged.sel(dict(time=slice('2022-08-15T09:00:00.000000000', '2022-08-30T11:00:00.000000000'),\n",
    "                                     time_sen=slice('2022-08-15T09:00:00.000000000', '2022-08-30T11:00:00.000000000'),\n",
    "                                     time_start=slice('2022-08-15T09:00:00.000000000', '2022-08-30T11:00:00.000000000')))\n",
    "\n",
    "#Add in metadata from .hdr file and project information\n",
    "\n",
    "headerFile1 = 'ADV/DEP204.hdr'\n",
    "headerFile2 = 'ADV/DEP205.hdr'\n",
    "metadata_list = [('Creator', 'Logan A. Grady'), ('Contact information', 'logan.grady@sjsu.edu'), ('Description', 'ADV deployed in Stillwater Cove, recording ENU velocity at 32Hz'),\n",
    " ('Lat', 36.56195999999164), ('Lon', -121.94174126537672)]\n",
    "\n",
    "ds1_cleaned = vt.vector_metadata(adv1_trimmed, headerFile1, metadata_list)\n",
    "ds2_cleaned = vt.vector_metadata(adv2_trimmed, headerFile2, metadata_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export usable data\n",
    "ds1_cleaned.to_netcdf('ADV/adv1All.nc')\n",
    "ds2_cleaned.to_netcdf('ADV/adv2All.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Look at raw adv data\n",
    "vecDS = adv1.copy(deep=True)\n",
    "East = vecDS.East.resample(time='20Min').mean().dropna(dim = 'time')\n",
    "North = vecDS.North.resample(time='20Min').mean().dropna(dim = 'time')\n",
    "Press = vecDS.Pressure.resample(time='20Min').std().dropna(dim = 'time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adv_qc_plot(vecData):\n",
    "    vecDS = vecData.copy(deep=True)\n",
    "    Cavg = (vecDS.Corr1 + vecDS.Corr2 + vecDS.Corr3)/3\n",
    "\n",
    "    fig, axs = plt.subplots(5,1,constrained_layout=True, figsize = (15,12))\n",
    "    \n",
    "    axs[0].set_title('Eastern Velocity')\n",
    "    axs[0].plot(vecDS.time, vecDS.East, '.b')\n",
    "    axs[0].axhline(y=0, c = 'black', ls = '--')\n",
    "    axs[0].set_ylabel('Velocity [m/s]')\n",
    "    axs[0].margins(x=.01)\n",
    "\n",
    "    axs[1].set_title('Average Beam Correlation')\n",
    "    axs[1].plot(vecDS.time, Cavg, '.r')\n",
    "    axs[1].set_ylabel('Correlation (%)')\n",
    "    axs[1].margins(x=.01)\n",
    "\n",
    "    axs[2].set_title('Instrument Tilt')\n",
    "    axs[2].plot(vecDS.time_sen, vecDS.Pitch, '.k', label = 'Pitch')\n",
    "    axs[2].plot(vecDS.time_sen, vecDS.Roll, '.g', label = 'Roll')\n",
    "    axs[2].set_ylabel('Degrees')\n",
    "    axs[2].margins(x=.01)\n",
    "    axs[2].legend(loc = 'upper left')\n",
    "\n",
    "    axs[3].set_title('Battery Voltage')\n",
    "    axs[3].plot(vecDS.time_sen, vecDS.BatVolt, '.y')\n",
    "    axs[3].set_ylabel('Voltage')\n",
    "    axs[3].margins(x=.01)\n",
    "\n",
    "    axs[4].set_title('Flag Score (1 = Pass, 3 = Suspect, 4 = Fail, 9 = Critical Fail)')\n",
    "    axs[4].plot(vecDS.time, vecDS.DataFlag, '.r', label = 'Velocity Data Flag')\n",
    "    axs[4].plot(vecDS.time_sen, vecDS.SenFlag, '.b', label = 'Sensor Data Flag')\n",
    "    axs[4].set_ylabel('Grade (1-9)')\n",
    "    axs[4].set_xlabel('Date')\n",
    "    axs[4].margins(x=.01)\n",
    "    axs[4].set_ylim(0,10)\n",
    "    axs[4].set_yticks([1,3,4,9])\n",
    "    axs[4].legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import usable data\n",
    "adv1 = xr.open_dataset('ADV/adv1All.nc')\n",
    "adv2 = xr.open_dataset('ADV/adv2All.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESPIKING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Despike adv data using the expanding ellipsoid method from Wheeler & Giddings 2023\n",
    "\n",
    "#Conditions\n",
    "badSections = [] #Bad sectiions already removed during data import\n",
    "reverse = False #Set variable for reversing direction after rotation if needed\n",
    "\n",
    "#Run expanding threshold despiking algorith and clean data with bad SNR/Correlation\n",
    "adv1Despiked = vt.ProcessVec(adv1,badSections,reverse)\n",
    "adv2Despiked = vt.ProcessVec(adv2,badSections,reverse)\n",
    "\n",
    "#'ProcessVec' function from Wheeler & Giddings has been modified to work with my dataset\n",
    "#but the core algorithm remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export despiked data\n",
    "adv1Despiked.to_netcdf('ADV/adv1_despiked.nc')\n",
    "adv2Despiked.to_netcdf('ADV/adv2_despiked.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repairing data gaps\n",
    "- Three methods\n",
    "    - Full linear interpolation across all gaps\n",
    "    - Partial interpolation over gaps <= 1s and averaging longer gaps with the average of the removed data\n",
    "    - Partial interpolation over gaps <= 1s and patching longer gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import despiked data\n",
    "adv1Despiked = xr.open_dataset('ADV/adv1_despiked.nc')\n",
    "adv2Despiked = xr.open_dataset('ADV/adv2_despiked.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linearly interpolating dataset\n",
      "Evaluating ratio of nans leftover in the dataset\n",
      "Linearly interpolating dataset\n",
      "Evaluating ratio of nans leftover in the dataset\n"
     ]
    }
   ],
   "source": [
    "#Full linear interpolation of gaps\n",
    "adv1Int = vt.fullInterpVec(adv1Despiked)\n",
    "adv2Int = vt.fullInterpVec(adv2Despiked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolating gaps <= 1s\n",
      "Evaluating ratio of nans leftover in the dataset\n",
      "Interpolating gaps <= 1s\n",
      "Evaluating ratio of nans leftover in the dataset\n"
     ]
    }
   ],
   "source": [
    "#Inerpolate gaps <= 1s and and patch the remaining gaps\n",
    "adv1Patch = vt.patchVec(adv1Despiked)\n",
    "adv2Patch = vt.patchVec(adv2Despiked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate gaps <= 1s and average the remaining gaps\n",
    "adv1IntAvg = vt.interpAvgVec(adv1Despiked)\n",
    "adv2IntAvg = vt.interpAvgVec(adv2Despiked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavenumber integral Jlm\n",
    "- Takes equation A13 from Gerbi et al. (2009) and applies it to the despiked and repaired datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the wavenumber integral for all datasets\n",
    "Jlm1Int = vt.JlmIntegral(adv1Int)\n",
    "Jlm2Int = vt.JlmIntegral(adv2Int)\n",
    "Jlm1Patch = vt.JlmIntegral(adv1Patch)\n",
    "Jlm2Patch = vt.JlmIntegral(adv2Patch)\n",
    "Jlm1IntAvg = vt.JlmIntegral(adv1IntAvg)\n",
    "Jlm2IntAvg = vt.JlmIntegral(adv2IntAvg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Jlm to corresponding datasets\n",
    "\n",
    "#For deployment 1 data\n",
    "adv1Int['J33'] = Jlm1Int['J_33']\n",
    "adv1Patch['J33'] = Jlm1Patch['J_33']\n",
    "adv1IntAvg['J33'] = Jlm1IntAvg['J_33']\n",
    "\n",
    "#For deployment 2 data\n",
    "adv2Int['J33'] = Jlm1Int['J_33']\n",
    "adv2Int['J33'] = Jlm2Patch['J_33']\n",
    "adv2IntAvg['J33'] = Jlm2IntAvg['J_33']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the Organized, cleaned, despiked, gap-repaired datasets with wavenumber integrals included\n",
    "adv1Int.to_netcdf('ADV/adv1Int.nc')\n",
    "adv2Int.to_netcdf('ADV/adv2Int.nc')\n",
    "\n",
    "adv1Patch.to_netcdf('ADV/adv1Patch.nc')\n",
    "adv2Patch.to_netcdf('ADV/adv2Patch.nc')\n",
    "\n",
    "adv1IntAvg.to_netcdf('ADV/adv1IntAvg.nc')\n",
    "adv2IntAvg.to_netcdf('ADV/adv2IntAvg.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import deployment 1 data\n",
    "adv1Int = xr.open_dataset('ADV/adv1_Interp.nc')\n",
    "adv1Int['J33'] = (['burst'], pd.read_csv('ADV/Jlm1Int.csv').J_33)\n",
    "adv1Patch = xr.open_dataset('ADV/adv1_Patched.nc')\n",
    "adv1Patch['J33'] = (['burst'], pd.read_csv('ADV/Jlm1Patch.csv').J_33)\n",
    "adv1IntAvg = xr.open_dataset('ADV/adv1_IntAverage.nc')\n",
    "adv1IntAvg['J33'] = (['burst'], pd.read_csv('ADV/Jlm1IntAvg.csv').J_33)\n",
    "\n",
    "#Import deployment 2 data\n",
    "adv2Int = xr.open_dataset('ADV/adv2_Interp.nc')\n",
    "adv2Int['J33'] = (['burst'], pd.read_csv('ADV/Jlm2Int.csv').J_33)\n",
    "adv2Patch = xr.open_dataset('ADV/adv2_Patched.nc')\n",
    "adv2Int['J33'] = (['burst'], pd.read_csv('ADV/Jlm2Patch.csv').J_33)\n",
    "adv2IntAvg = xr.open_dataset('ADV/adv2_IntAverage.nc')\n",
    "adv2IntAvg['J33'] = (['burst'], pd.read_csv('ADV/Jlm2IntAvg.csv').J_33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If phase wrapping is present\n",
    "- Convert velocities from ENU to beam by using the transformation matrix in .hdr files\n",
    "    - The following code is based on a MatLab script available on NORTEK's FAQ forums:\n",
    "        https://support.nortekgroup.com/hc/en-us/articles/360029820971-How-is-a-coordinate-transformation-done-\n",
    "        - Most relevant information is available in the .hdr file\n",
    "        - You will need the transformation matrix, as well as heading, pitch, and roll data for each sample to make the conversions\n",
    "    - Once velocities have been converted, calculate the ambiguous velocity V_amb\n",
    "    - Run a patch over the entire BEAM velocity dataset\n",
    "        - If phase wrap is negative: newvel = oldvel + 2*V_amb\n",
    "        - If phase wrap is positive: newvel = oldvel - 2*V_amb\n",
    "    -Convert patched velocities back to ENU for more user-friendly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation matrix located in .hdr file as 'Transformation matrix'\n",
    "\n",
    "T = np.array([[2.7249, -1.3770, -1.3503], #Convert matrix to multidimensional numpy array\n",
    "   [-0.0161, 2.3442, -2.3308],\n",
    "   [0.3472, 0.3455, 0.3389]])\n",
    "\n",
    "# Heading, pitch and roll are the angles output in the data in degrees\n",
    "# Convert to radians\n",
    "hh = np.pi*(adv1_flagged['Heading']-90)/180 #Creates list of hh, pp, and rr for all datapoints\n",
    "pp = np.pi * (adv1_flagged['Pitch']/180)\n",
    "rr = np.pi * (adv1_flagged['Roll']/180)\n",
    "\n",
    "# Generate empty arrays to be populated by resulting beam velocities\n",
    "beam1 = np.empty(len(adv1_flagged)) # Already created to be the length of the dataset to save processing time\n",
    "beam2 = np.empty(len(adv1_flagged))\n",
    "beam3 = np.empty(len(adv1_flagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate heading matrix and tilt matrix for each data point and convert ENU velocities to beam velocities\n",
    "\n",
    "# for loop iterates for each data point\n",
    "for i in range(0,len(adv1_flagged)):\n",
    "    if i % 1000000 == 0: # Progress check every 1000000 rows\n",
    "                print('Currently on row:', i)\n",
    "            \n",
    "    H = np.array([[np.cos(hh[i]), np.sin(hh[i]), 0], # Makes the heading matrix for row i\n",
    "                  [-np.sin(hh[i]), np.cos(hh[i]), 0], \n",
    "                  [0, 0, 1]])\n",
    "    \n",
    "    P = np.array([[np.cos(pp[i]), -np.sin(pp[i])*np.sin(rr[i]), -np.cos(rr[i])*np.sin(pp[i])], # Makes the tilt matrix for row i\n",
    "                  [0, np.cos(rr[i]), -np.sin(rr[i])], \n",
    "                  [np.sin(pp[i]), np.sin(rr[i])*np.cos(pp[i]), np.cos(pp[i])*np.cos(rr[i])]])\n",
    "    \n",
    "    R = H*P*T #Product of transformation, heading, and tilt matrix creates conversion matrix R\n",
    "\n",
    "    # Retrieves ENU velocities from row i to be converted by R[i]\n",
    "    enu = np.array([adv1_flagged['Velocity_East(m/s)'][i], adv1_flagged['Velocity_North(m/s)'][i], adv1_flagged['Velocity_Up(m/s)'][i]])\n",
    "    \n",
    "    beam1[i] = np.dot(np.linalg.inv(R),enu)[0]\n",
    "    beam2[i] = np.dot(np.linalg.inv(R),enu)[1]\n",
    "    beam3[i] = np.dot(np.linalg.inv(R),enu)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate v_amb\n",
    " - v_amb = VR * 2\n",
    " - VR = c/(4 * f * tlag(s))\n",
    " - c = 1530 (speed of sound measured by instrument)\n",
    " - f = instrument frequency (6000kHz)\n",
    " - tlag (for vector) = 50/480000 (50 is from system 38 in .hdr file, which indicates nominal velocity of 1m/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
